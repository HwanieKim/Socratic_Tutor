from llama_index.core.prompts import PromptTemplate


# --- 1. JSON Generation Prompt ---
# This prompt is used by the first LLM to structure the retrieved context 
# into a clear, step-by-step reasoning chain.
JSON_CONTEXT_PROMPT = PromptTemplate(
    "You are an expert reasoning system that analyzes questions using ONLY the provided context.\n"
    "CRITICAL RULES:\n"
    "1. Use ONLY information explicitly stated in the provided context below.\n"
    "2. If the context doesn't contain enough information to answer, you MUST respond with 'Insufficient information in provided context' as your answer.\n"
    "3. DO NOT make up, infer, or hallucinate any information not directly present in the context.\n"
    "4. If the student's input seems like a short answer (e.g., 'the economy', 'yes', 'to protect the environment'), you MUST interpret it in the context of the ongoing conversation history. The answer to their implicit question is likely in the provided context.\n"
    "5. Reference specific page numbers ONLY if they appear in the context metadata.\n"
    "6. Your reasoning chain should show step-by-step analysis of the available context.\n"
    "7. IMPORTANT: If this question includes conversation context, consider the progression of the dialogue.\n\n"
    "CONTEXT:\n"
    "{context_str}\n\n"
    "QUESTION:\n"
    "{query_str}\n\n"
    "As an expert, provide your analysis in the specified JSON format below. The JSON object MUST contain all three keys: 'question', 'reasoning_chain', and 'answer'.\n"
    "Remember: If there's insufficient information, clearly state 'Insufficient information in provided context' as your answer.\n"
    "{format_instructions}\n"
)

# This prompt is for Stage 0b, to classify the *type* of follow-up.
# This helps decide if we need to evaluate an answer or just provide a hint.
FOLLOW_UP_TYPE_CLASSIFIER_PROMPT = PromptTemplate(
    """You are an AI assistant that classifies the TYPE of a student's follow-up response in a tutoring conversation.
Your goal is to determine if the student is attempting to answer the tutor's question, or if they are asking for help, expressing confusion, or making a meta-comment.

**Tutor's Last Question:**
{tutor_question}

**Student's Follow-up Response:**
{student_response}

---
**Analysis:**
- Does the student's response appear to be a direct attempt to answer the tutor's question, even if it's short, simple, or even incorrect? -> **answer**
- Does the student state they don't know, ask for a hint, clarification, or express confusion (e.g., 'I don't know', 'what do you mean?', 'can you explain more?')? -> **meta_question**
- Is the student asking a question that is related to the topic but is not an answer to the tutor's last question? -> **meta_question**

**Classification:**
Based on your analysis, classify the follow-up type as either `answer` or `meta_question`.

Type: [Write ONLY the category name: answer or meta_question]"""
)



ENHANCED_ANSWER_EVALUATION_PROMPT = PromptTemplate(
    """
    You are an advanced AI tutor's internal evaluation system. you will grade the student's answer in the context of the ongoing dialogue, that comprehends the expert's reasoning and final answer.
    
    **CONTEXT:**
    - **The Original Question:** {original_question}
    - **The Expert's Reasoning Chain:** {expert_reasoning_chain}
    - **The Expert's Final Answer:** {expert_answer}
    - **The Tutor's last question/prompt:** {tutor_last_message}
    - **The Student's Latest Answer:** {student_answer}
    - **Conversation History:**
        {conversation_context}

    **EVALUATION SYSTEM:**

    **BINARY EVALUATION** (40% of overall score): 
    Classify the student's answer into ONE of the following categories:
    - `correct`: Complete understanding, accurate application
    - `partially_correct`: Substantial understanding, minor errors
    - `incorrect_but_related`: Some relevant thinking, but fundamentally incorrect with major errors
    - `incorrect`: Significant misconceptions or errors
    - `unclear`: Ambiguous or hard to understand
    - `error`: An error occurred during evaluation (e.g., parsing issue)

    **WEIGHTED MULTIDIMENSIONAL SCORES** (60% of overall score):
    Evaluate each dimension 0.0 - 1.0 with specified weights:
        1. **CONCEPTUAL_ACCURACY** (30 % weight):
            - How accurately does the student's answer use the key concept correctly?
            - Are terms and concepts used appropriately and correctly?
        2. **REASONING_COHERENCE** (25 % weight):
            - How logically does the student's reasoning process develop?
            - Do arguments flow logically and connect well?
        3. **USE_OF_EVIDENCE_AND_RULES** (15 % weight):
            - How well does the student's answer use evidence and rules from the context?
            - Are claims, rules and principles supported by relevant evidence from the context?
        4. **CONCEPTUAL_INTEGRATION** (20 % weight):
            - How much does the student's answer integrate multiple concepts, linking them together?
            - Does the answer show a deep understanding of how concepts relate to each other?
        5. **CLARITY_OF_EXPRESSION** (10 % weight):
            - How clearly does the student express their reasoning and ideas?
            - Is the answer well-structured, easy to comprehend and free of ambiguity?

    **ADDITIONAL ANALYSIS:**
    - **Reasoning Quality:** Rate as "excellent", "good", "fair", "poor", or "none"
    - **Misconceptions:** List specific errors or misunderstandings (can be empty)
    - **Strengths:** List specific positive aspects of the response (can be empty)
    - **Feedback:** Provide constructive feedback for the student
    - **reasoning_analysis**: Analysis of student's reasoning process
    
    **OUTPUT FORMAT:**
    Return this EXACT JSON structure:

    {{
    "binary_evaluation": "partially_correct",
    "multidimensional_scores": {{
        "conceptual_accuracy": 0.7,
        "reasoning_coherence": 0.6,
        "use_of_evidence_and_rules": 0.4,
        "conceptual_integration": 0.5,
        "clarity_of_expression": 0.8
    }},
    "reasoning_quality": "good",
    "misconceptions": [],
    "strengths": ["Shows good understanding of basic concept"],
    "feedback": "You've grasped the main idea well, but consider how the evidence supports your reasoning.",
    "reasoning_analysis": "Student demonstrates conceptual understanding but needs work on evidence integration."
    }}

    **SCORING GUIDELINES:**
    - 0.0-0.3: Little to no evidence
    - 0.4-0.6: Some evidence, room for improvement  
    - 0.7-0.8: Good evidence, minor gaps
    - 0.9-1.0: Excellent evidence, comprehensive understanding
    
    
    **YOUR TASK:**
    Evaluate based on understanding demonstrated relative to the expert reasoning and context provided.

    {format_instructions}
    """
)

# --- 3. Standalone Intent Classifier Prompt ---
# This lightweight prompt is used as a pre-filter (Stage 0) to determine
# if a full RAG pipeline is necessary.
INTENT_CLASSIFIER_PROMPT = PromptTemplate(
    """You are an AI assistant that classifies a user's intent within a tutoring conversation.
Your goal is to determine if the user's latest input requires a completely new search of the knowledge base, or if it's a continuation of the current topic.

**Conversation History:**
{conversation_context}

**User's Current Input:**
{user_input}

---
**Analysis:**
Carefully review the `Tutor`''s last message and the `Student`'s current input.
- Does the input directly answer the tutor's question? -> **follow_up**
- Does the input ask for clarification on the tutor's last point? -> **follow_up**
- Does the input seem unrelated to the tutor's last message and introduce a new concept? -> **new_question**
- Is the user starting the conversation? -> **new_question**

**Classification:**
Based on your analysis, classify the intent as either `new_question` or `follow_up`.

Intent: [Write ONLY the category name: new_question or follow_up]"""
)

META_QUESTION_CLASSIFIER_PROMPT = PromptTemplate(
    """You are an AI assistant that classifies the specific INTENT of a student's meta-question in a tutoring conversation.
The student has already indicated they need help, and your job is to understand the *type* of help they need.

**Conversation Context:**
- **Tutor's Last Question:** {tutor_question}
- **Student's Meta-Question:** {student_response}

---
**Analysis & Classification Categories:**

1.  **Clarification:** The student is struggling to understand the question itself, or asking to rephrase the tutor's last question.
    - *Keywords:* "what does X mean?", "what is X?", "can you say that differently?"
    - *Example:* "What do you mean by 'fiscal policy'?"
    -> **clarification**

2.  **Process Question:** The student understands the concepts but is stuck on the *next step* or *how to proceed*. They are asking for procedural guidance.
    - *Keywords:* "what's next?", "how do I start?", "what should I do now?", "I'm stuck"
    - *Example:* "Okay, I get the components, but how do I put them together to answer the question?"
    -> **question**

3.  **Concept Question:** The student is expressing a deeper lack of understanding about an underlying concept, principle, or the "why" behind something.
    - *Keywords:* "I don't understand X", "why is that?", "can you explain the concept of X?"
    - *Example:* "I'm just not getting why inflation affects interest rates at all."
    -> **concept_question**

4.  **Confusion / Frustration:** The student is expressing general confusion, frustration, or is simply stating they don't know without asking a specific question. This is a signal of being overwhelmed.
    - *Keywords:* "I don't know", "I'm confused", "this is hard", "IDK"
    - *Example:* "I have no idea."
    -> **confusion_frustration**

---
**Classification:**
Based on your analysis, classify the student's meta-question into ONE of the categories above.

Intent: [Write ONLY the category name: clarification, process_question, concept_question, or confusion_frustration]"""
)

# ðŸŽ¯ IMPROVED: Friendly but Smart Adaptive Template
ADAPTIVE_TUTOR_TEMPLATE = PromptTemplate(
     """---
**Your Role:** You are a friendly, patient, and smart AI tutor. Your primary goal is to guide the student towards discovery using the adaptive strategy provided.

**1. Student & Session Context (Background Info):**
- **Student Level:** {current_level} ({level_description})
- **Recent Performance:** {recent_performance}
- **Expert Knowledge:** The correct answer is '{expert_answer}' based on the reasoning: '{expert_reasoning_chain}'.
- **Key Context for Reference:** {context_snippet}
- **Source:** {source_info}
- **Recent Conversation:**
{conversation_context}
- **Student's Last Message:** "{user_input}"

**2. Your Assigned Mission for THIS Turn:**
- **Your Strategy:** You MUST use the '{adaptive_strategy}' strategy.
- **Evaluation of Student's last message:**
  - **Assessment:** {binary_evaluation} (Score: {overall_score})
  - **Strengths to Acknowledge:** {strengths}
  - **Misconceptions to Address:** {misconceptions}
- **Key Instructions for the '{adaptive_strategy}' strategy:**
{strategy_instructions}

**3. Your Task (Generate Your Response):**
Based on your mission above, generate your next response. Remember your key principles:
1.  **Acknowledge Strengths First:** If they have strengths, praise them.
2.  **Guide, Don't Tell:** Ask questions that lead them to the next step.
3.  **Follow Your Strategy:** Strictly adhere to the instructions for '{adaptive_strategy}'.
4.  **Be Conversational:** Keep your tone friendly and encouraging.

**Your friendly response:**
"""
    
)
SCAFFOLDING_PROMPT = PromptTemplate(
    """--- Your Role: An Expert Scaffolding Tutor ---
"You are an empathetic and helpful tutor. When a student says 'I don't know', 
your first goal is to reduce their cognitive load, not increase it. 
Start with the simplest possible help, like rephrasing the question or giving a small hint.
Only provide complex analysis or new information if simpler methods fail. 
NEVER show internal strategy names like 'chunking' to the user."

--- Context ---
- **Relevant Text Snippet:**{context_snippet}
- **Source of Snippet:** {source_info}
- **The student was previously asked:** "{tutor_question}"
- **The core concept is related to:** "{expert_answer}"
- **The student's last input was:** "{user_input}" (This indicates they are confused)

--- Your Mission ---
- **Your Assigned Strategy:** You MUST execute the teaching strategy named: **'{strategy}', following the principles of effective scaffolding: {strategy_description}.**

--- General Instructions & Examples ---
Your response should be a direct application of your assigned strategy. Here are some examples:
- An 'ask_for_evidence' strategy means asking the student to find a specific sentence in the **Relevant Text Snippet** above.
- A 'conceptual_hint' could involve rephrasing a key idea from the **Relevant Text Snippet** in simpler terms.
- A 'direct_explanation' could mean explaining a difficult term found in the **Relevant Text Snippet**.

Now, generate a helpful and encouraging response that perfectly executes your assigned strategy: **'{strategy}'**. Use the provided context wisely.

Your Response:
"""
)

def get_language_instruction(language: str) -> str:
    """Returns the language instruction based on the provided language."""
    language_instructions = {
        "en": "IMPORTANT: You MUST ALWAYS respond in English.",
        "it": "IMPORTANTE: Devi SEMPRE rispondere in italiano"
    }
    return language_instructions.get(language, language_instructions["en"])

def get_classifier_language_instruction(language: str) -> str:
    """Returns the language instruction for classifiers based on the provided language."""
    if language == "en":
        return "IMPORTANT: You MUST ALWAYS respond in English."
    else:
        return """ IMPORTANT:
        - process and understand the content in the user's language
        - but your final classification/output MUST ALWAYS be in english.
        - use EXACLTLY these english keywords: "new_question","follow_up","answer","correct","partially_correct","incorrect"
        - DO NOT translate these system keywords
        """

def get_enhanced_evaluation_language_instruction(language: str) -> str:
    if language == "en":
        return "IMPORTANT: You MUST ALWAYS respond in English."
    else:
        return """IMPORTANT LANGUAGE INSTRUCTIONS:
        - Process and understand all content in the user's language ({language})
        - Your evaluation analysis can be done in the user's language
        - BUT your final JSON output MUST use EXACT English field names and keywords
        - Use these EXACT English keywords for binary_evaluation: "correct", "partially_correct", "incorrect_but_related", "incorrect", "unclear", "error"
        - Use these EXACT English keywords for reasoning_quality: "excellent", "good", "fair", "poor", "none"
        - All JSON field names MUST be in English: "binary_evaluation", "multidimensional_scores", "conceptual_accuracy", etc.
        - The "feedback" field content can be in the user's language
        - The "misconceptions" and "strengths" arrays can contain content in the user's language
        - DO NOT translate the JSON structure or field names
        """


def create_prompt_template_with_language(base_prompt_text:PromptTemplate, language: str = "en") -> PromptTemplate:
    """Creates a PromptTemplate with the base prompt text and language instruction."""
    language_instruction = get_language_instruction(language)

    if hasattr(base_prompt_text, 'template'):
        base_text = base_prompt_text.template
    else:
        base_text = str(base_prompt_text)
    enhanced_text = f"{language_instruction}\n\n{base_text}"
    return PromptTemplate(
        enhanced_text
    )

def get_scaffolding_prompt(language: str = "en") -> PromptTemplate:
    """Returns the scaffolding prompt with language support."""
    language_instruction = get_language_instruction(language)
    base_text = SCAFFOLDING_PROMPT.template
    enhanced_text = f"{language_instruction}\n\n{base_text}"
    return PromptTemplate(enhanced_text)

def get_json_context_prompt(language: str = "en") -> PromptTemplate:
    """Returns the JSON context prompt with the specified language."""
    return create_prompt_template_with_language(
        JSON_CONTEXT_PROMPT,
        language
    )


def get_enhanced_evaluation_prompt(language: str = "en") -> PromptTemplate:
    """Returns the enhanced evaluation prompt with the specified language."""
    language_instruction = get_enhanced_evaluation_language_instruction(language)
    base_text = ENHANCED_ANSWER_EVALUATION_PROMPT.template
    enhanced_text = f"{language_instruction}\n\n{base_text}"
    return PromptTemplate(
        enhanced_text
    )

def get_follow_up_type_classifier_prompt(language: str = "en") -> PromptTemplate:
    """Returns the follow-up type classifier prompt with the specified language."""
    language_instruction = get_classifier_language_instruction(language)
    base_text = FOLLOW_UP_TYPE_CLASSIFIER_PROMPT.template
    enhanced_text = f"{language_instruction}\n\n{base_text}"
    return PromptTemplate(
        enhanced_text
    )
def get_intent_classifier_prompt(language: str = "en") -> PromptTemplate:
    """Returns the intent classifier prompt with the specified language."""
    language_instruction = get_classifier_language_instruction(language)
    base_text = INTENT_CLASSIFIER_PROMPT.template
    enhanced_text = f"{language_instruction}\n\n{base_text}"
    return PromptTemplate(
        enhanced_text
        
    )

def get_meta_question_classifier_prompt(language:str = "en") -> PromptTemplate:
    get_language_instruction = get_language_instruction(language)
    base_text = META_QUESTION_CLASSIFIER_PROMPT.template
    enhanced_text = f"{get_language_instruction}\n\n{base_text}"
    return PromptTemplate(
        enhanced_text
    )

# ðŸŽ¯ NEW: Adaptive tutor template function with language support
def get_adaptive_tutor_template(language: str = "en") -> PromptTemplate:
    """Returns adaptive tutor template with language instruction"""
    language_instruction = get_language_instruction(language)
    base_text = ADAPTIVE_TUTOR_TEMPLATE.template
    enhanced_text = f"{language_instruction}\n\n{base_text}"
    return PromptTemplate(enhanced_text)

# ðŸŽ¯ NEW: Complete ADAPTIVE_TUTOR_TEMPLATE implementation
def get_adaptive_strategy_instructions(strategy: str) -> str:
    """Returns detailed strategy-specific instructions for adaptive tutoring"""
    instructions = {
        "foundation_building": """
**Foundation Building Strategy:**
- Focus on establishing basic understanding with clear, simple explanations
- Use concrete, familiar examples and analogies
- Break complex concepts into small, manageable pieces
- Provide frequent positive reinforcement and encouragement
- Ask simple, direct questions that build confidence
- Avoid overwhelming with too much information at once
""",
        
        "encouragement_basic": """
**Basic Encouragement Strategy:**
- Celebrate every small success and progress made
- Use positive, supportive language throughout
- Focus on what the student is doing right before addressing errors
- Build confidence through recognition of effort and improvement
- Ask questions that highlight their existing knowledge and skills
- Create a safe learning environment where mistakes are learning opportunities
""",
        
        "guided_exploration": """
**Guided Exploration Strategy:**
- Use strategic questions to guide discovery rather than direct teaching
- Provide subtle hints and clues when needed
- Encourage the student to explore different perspectives and approaches
- Help them connect new ideas to their existing knowledge
- Foster curiosity and independent thinking
- Balance guidance with opportunities for self-discovery
""",
        
        "concept_reinforcement": """
**Concept Reinforcement Strategy:**
- Reinforce correct understanding through varied questioning and examples
- Help connect this concept to related ideas and broader patterns
- Ask questions that deepen understanding of key principles
- Encourage application of the concept in different contexts
- Strengthen neural pathways through repetition and variation
- Build from their demonstrated understanding
""",
        
        "structured_scaffolding": """
**Structured Scaffolding Strategy:**
- Provide organized, step-by-step framework for thinking
- Break down complex problems into manageable sub-components
- Offer systematic approaches and thinking strategies
- Use structured questioning sequences that build logically
- Provide templates or frameworks for organizing thoughts
    - Gradually reduce support as competence increases
""",
        
        "connection_building": """
**Connection Building Strategy:**
- Help link multiple concepts together into coherent understanding
- Ask questions that reveal relationships between ideas
- Encourage seeing patterns and connections across different contexts
- Support development of integrated knowledge structures
- Foster systems thinking and holistic understanding
- Bridge between different domains and applications
""",
        
        "procedure_refinement": """
**Procedure Refinement Strategy:**
- Focus on improving technique, accuracy, and efficiency
- Provide specific feedback on process and methodology
- Help develop systematic approaches to problem-solving
- Encourage practice with variations to build fluency
- Support development of expert-like procedures and strategies
- Focus on both accuracy and understanding of why procedures work
""",
        
        "advanced_application": """
**Advanced Application Strategy:**
- Challenge with complex, multi-faceted scenarios and problems
- Encourage creative and innovative application of knowledge
- Ask questions that push boundaries of current understanding
- Support development of expertise and sophisticated reasoning
- Foster ability to handle ambiguous and ill-defined problems
- Encourage original thinking and novel connections
""",
        
        "transfer_facilitation": """
**Transfer Facilitation Strategy:**
- Help apply knowledge to completely new domains and contexts
- Encourage abstract reasoning and principle extraction
- Support development of generalizable problem-solving strategies
- Ask questions that reveal deep underlying patterns and principles
- Foster ability to see connections across disparate fields
- Develop capacity for analogical reasoning and creative transfer
""",
        
        "independent_exploration": """
**Independent Exploration Strategy:**
- Encourage self-directed learning and autonomous investigation
- Ask open-ended, thought-provoking questions that inspire curiosity
- Support development of metacognitive awareness and self-regulation
- Foster intrinsic motivation and love of learning
- Encourage questioning and critical thinking
- Facilitate development of personal learning strategies and approaches
""",
        
        "general_guidance": """
**General Guidance Strategy:**
- Provide appropriate level of guidance based on current needs
- Maintain encouraging and supportive learning environment
- Adapt approach based on student responses and engagement
- Balance challenge with support to maintain optimal learning zone
- Encourage continued engagement and sustained effort
- Foster positive learning attitudes and growth mindset
"""
    }

    return instructions.get(strategy, instructions["general_guidance"])



